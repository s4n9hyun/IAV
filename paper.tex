
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Modular Alignment Vectors: A Plug-and-Play Framework for Test-Time LLM Alignment}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Sanghyun Lee, Hoh Peter In \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Korea University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Large Language Models (LLMs) require alignment with human preferences to be safe and useful. Current state-of-the-art test-time alignment methods, while avoiding costly retraining, depend on a separate, external reward model that runs in parallel with the base LLM, effectively doubling inference costs and memory requirements. This architectural bottleneck presents a significant barrier to practical and efficient deployment. To address this fundamental limitation, we introduce Inherent Alignment Vectors (IAV), a new paradigm that eliminates the need for an external reward model. IAV is based on a unified architecture where a single LLM learns to generate both its standard next-token predictions and a corrective 'alignment vector' from its own internal representations. We propose a novel training objective that intentionally decouples the model's learned knowledge from its alignment policy, enabling dynamic, steerable control over the generation process at inference time. Our framework is stabilized by a principled regularization scheme that prevents potential training collapse scenarios. Experiments demonstrate that IAV achieves alignment performance competitive with state-of-the-art methods like DPO and GenARM, while reducing inference compute and memory usage by nearly 50\%. This work presents a significant breakthrough towards truly efficient, accessible, and controllable LLM alignment.
\end{abstract}
\section{Introduction}

The alignment of Large Language Models (LLMs) with human values is a critical prerequisite for their safe and beneficial deployment. While training-time methods like Direct Preference Optimization (DPO) \citep{rafailov2023direct} produce high-quality static models, they cannot adapt to new or diverse user preferences without expensive retraining. To overcome this rigidity, Test-Time Alignment (TTA) has emerged as a promising alternative, guiding a frozen LLM during inference \citep{khanov2024args, xu2024genarm}.

However, state-of-the-art TTA methods such as GenARM \citep{xu2024genarm} introduce a fundamental architectural bottleneck: they require running a separate, large reward model in parallel with the base LLM. This "two-model problem" doubles the memory footprint and significantly increases computational latency, creating a major obstacle for practical, large-scale deployment.

In this paper, we propose a new paradigm that retains the flexibility of TTA while resolving its efficiency challenges. We introduce \textbf{Modular Alignment Vectors (MAV)}, a novel plug-and-play framework for test-time alignment. The core idea is to completely decouple the base LLM (the "knowledge" component) from a separate, lightweight \textbf{Alignment Model} (the "values" component). This Alignment Model is trained only once on preference data and learns to generate a corrective "alignment vector" ($\mathbf{a}_t$) by observing the internal hidden states of any compatible, frozen base LLM.

At inference time, the frozen base model and the trained Alignment Model work in tandem. The Alignment Model's output vector is added to the base model's logits via a controllable coefficient ($\alpha$), allowing for real-time, steerable alignment without modifying the base model's weights. To enhance its capability, our Alignment Model fuses representations from multiple layers of the base model, giving it a deeper contextual understanding to produce more nuanced alignment vectors.

Our contributions are as follows:
\begin{enumerate}
    \item We propose MAV, a new TTA framework that resolves the "two-model problem" by training a single, lightweight, and reusable Alignment Model that can be paired with any frozen base LLM.
    \item We introduce a multi-layer fusion mechanism within the Alignment Model, enabling it to leverage deeper contextual information from the base model for more effective alignment.
    \item We demonstrate that a single MAV Alignment Model can be used as a "plug-and-play" component across different base LLMs (e.g., Llama, Mistral), showcasing true modularity.
    \item We show empirically that MAV achieves competitive alignment performance with strong baselines while offering superior efficiency compared to traditional two-model TTA systems.
\end{enumerate}
\section{Related Work}
\subsection{Training-Time Alignment}
The dominant alignment paradigm involves modifying the model weights during training. Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022training} uses a reward model to fine-tune an LLM with RL algorithms like PPO. Direct Preference Optimization (DPO) \citep{rafailov2023direct} simplifies this pipeline by deriving a direct preference loss, removing the need for a complex RL stage. While powerful, these methods produce static models that cannot be dynamically adjusted to diverse user preferences at inference time, a limitation that motivates the need for TTA methods like IAV.

\subsection{Test-Time Alignment}
Our work is most directly situated within the paradigm of Test-Time Alignment (TTA), which aims to guide a frozen LLM during decoding. Early approaches like ARGS \citep{khanov2024args} pioneered reward-guided search but suffered from inaccuracies when applying trajectory-level reward models to partial sequences. This fundamental \textit{unit mismatch} problem was elegantly solved by the introduction of the Autoregressive Reward Model (ARM) in GenARM \citep{xu2024genarm}, which provides principled, token-level reward signals through logit ensembling.

Despite their success, these state-of-the-art methods are fundamentally constrained by their reliance on a separate, large ARM. This architectural choice doubles the memory and computational cost at inference, creating a significant barrier to practical deployment. This suggests that a more fundamental architectural and training methodology shift is needed to achieve efficiency without sacrificing performance. IAV provides such a shift by internalizing the alignment mechanism and introducing a novel training objective for stable decoupling.

\subsection{Representation Editing and Control}
A parallel and important line of research focuses on controlling LLM behavior through direct manipulation of internal representations. Methods like Representation Engineering (RepE) \citep{zou2023representation} and steering vectors \citep{turner2023steering} modify the model's hidden states to control abstract concepts such as honesty, power-seeking, or specific emotions.

\paragraph{Key Distinctions from IAV.} While both are forms of inference-time control, they operate at different levels and address different goals. We highlight three key differentiators:
\begin{itemize}
\item \textbf{Target of Intervention:} Representation editing manipulates internal \textbf{hidden states} across various layers to alter the model's "thought process." In contrast, IAV manipulates the final \textbf{output logits} to directly shape the next-token probability distribution.
\item \textbf{Objective \& Data:} Representation editing typically targets the control of specific, often abstract, \textit{concepts} using contrastive prompt pairs. IAV targets alignment with general human \textit{preferences} (e.g., helpfulness, harmlessness) using large-scale preference datasets.
\item \textbf{Primary Focus:} The primary goal of representation editing is the precision and interpretability of conceptual control. The primary goal of IAV is achieving dramatic \textbf{computational efficiency} in the preference alignment task.
\end{itemize}
These approaches are complementary rather than competing. The alignment vectors learned by IAV could potentially be combined with steering vectors for more nuanced control, representing an exciting direction for future work.

\subsection{Efficient LLM Inference}
Reducing the computational cost of LLM inference is a critical area of research. Techniques such as model quantization, knowledge distillation, and speculative decoding \citep{leviathan2023fast} have made significant strides. IAV contributes to this goal from a novel architectural perspective. Instead of compressing the model's weights, IAV redesigns the alignment process itself to eliminate an entire external model, thereby offering a new, orthogonal approach to achieving inference efficiency for aligned LLMs.
\section{The Modular Alignment Vector (MAV) Framework}

Our work is founded on the hypothesis that the alignment mechanism can be completely decoupled from the base language model. We introduce Modular Alignment Vectors (MAV), a novel plug-and-play framework for test-time alignment that instantiates this hypothesis. MAV consists of two distinct components: a frozen, off-the-shelf base LLM and a separate, lightweight Alignment Model that is trained to produce corrective vectors in the logit space.

\subsection{MAV Architecture: Decoupling Knowledge and Alignment}

The MAV architecture is composed of two primary, independent modules that operate in tandem at inference time, as detailed in our implementation in \texttt{mav\_models.py}.

\paragraph{1. The Frozen Base Model.}
The first component is a standard, pre-trained Supervised Fine-Tuned (SFT) LLM, which we refer to as the \texttt{FrozenBaseModel}. This model serves as the "knowledge" base of the system. As its name suggests, all of its parameters are completely frozen ($\texttt{requires\_grad=False}$) during the alignment training process. A key feature of our framework is its modularity; this base model can be switched at runtime to any other compatible LLM, allowing a single trained Alignment Model to guide various base models (e.g., swapping a Llama-7B for a Mistral-7B). The base model's role is to produce its standard next-token logits ($\mathbf{z}_{\text{base}}$) and, crucially, the internal hidden states from multiple specified layers.

\paragraph{2. The Trainable Alignment Model.}
The second component is the \texttt{AlignmentModel}, a separate, lightweight neural network that is the sole trainable module in the MAV framework. Its purpose is to learn a general alignment policy that can be applied to the outputs of the frozen base model. To generate a more contextually-aware correction, the Alignment Model leverages a multi-layer fusion approach:

\begin{itemize}
    \item \textbf{Multi-Layer Input:} Instead of only observing the final hidden state, the Alignment Model receives hidden states from multiple layers of the base model (e.g., first, middle, and last). This provides a richer, more hierarchical view of the base model's internal "thought process."
    \item \textbf{Adaptive Pooling:} To ensure that the Alignment Model is compatible with various base models of different hidden sizes, an \texttt{adaptive\_pool} layer first standardizes the dimensionality of the incoming hidden states to a fixed target size (e.g., 4096).
    \item \textbf{Fusion Mechanism:} The standardized hidden states from the different layers are then concatenated and passed through a \texttt{fusion\_layer}, which combines them into a single, rich representation.
    \item \textbf{Alignment Vector Generation:} Finally, this fused representation is passed through the main \texttt{alignment} network to produce the alignment vector, $\mathbf{a}_t$, which has the same dimension as the vocabulary size.
\end{itemize}

\subsection{Inference: Real-Time Plug-and-Play Alignment}

At inference time, the two modules work together to produce an aligned output. For each token generation step, the process is as follows, as implemented in \texttt{mav\_models.py}:
\begin{enumerate}
    \item The \texttt{FrozenBaseModel} takes the current input sequence and produces the base logits $\mathbf{z}_{\text{base}}$ and the multi-layer hidden states.
    \item These multi-layer hidden states are fed into the trained \texttt{AlignmentModel}, which produces the alignment vector $\mathbf{a}_t$.
    \item The final, aligned logits are computed via a simple additive combination, controlled by a scalar hyperparameter $\alpha$:
    \begin{equation}
        \mathbf{z}_{\text{final}} = \mathbf{z}_{\text{base}} + \alpha \cdot \mathbf{a}_t
    \end{equation}
    The next token is then sampled from the probability distribution derived from $\text{softmax}(\mathbf{z}_{\text{final}})$. The coefficient $\alpha$ allows for real-time, steerable control over the alignment strength at inference time.
\end{enumerate}

\subsection{Training Objective: Learning the Alignment Model}

The training process, detailed in \texttt{mav\_train.py}, is designed to be highly efficient by only updating the weights of the lightweight \texttt{AlignmentModel}. The training objective consists of two components:

\paragraph{Preference Loss.} The primary driver for learning is a DPO-style preference loss. Given a preference pair of chosen ($y_w$) and rejected ($y_l$) responses, the loss encourages the model to assign a higher probability to the chosen response:
\begin{equation}
    \mathcal{L}_{\text{pref}} = -\mathbb{E}_{ (x, y_w, y_l) \sim \mathcal{D} } \left[ \log \sigma \left( \beta \left( \log \pi_{\theta}(y_w|x) - \log \pi_{\theta}(y_l|x) \right) \right) \right]
\end{equation}
where $\pi_{\theta}$ is the policy derived from $\text{softmax}(\mathbf{z}_{\text{final}})$ and $\beta$ is a hyperparameter. To maximize training throughput, we employ a batch concatenation strategy, processing both chosen and rejected sequences in a single forward pass.

\paragraph{Alignment Vector Regularization.} To prevent the alignment vector from becoming unnecessarily large and to encourage the model to find the most efficient correction, we apply an L2 penalty on the norm of the alignment vector:
\begin{equation}
    \mathcal{L}_{\text{L2}} = ||\mathbf{a}_t||_2^2
\end{equation}
The final training objective is a weighted sum of these two components: $\mathcal{L}_{\text{MAV}} = \mathcal{L}_{\text{pref}} + \lambda_{L2} \mathcal{L}_{\text{L2}}$.
\section{Experiments}
We conduct a series of experiments to rigorously validate the core hypotheses of our MAV framework. Our experiments aim to answer four key questions: (1) Does MAV achieve competitive alignment performance against strong training-time and test-time baselines? (2) Does MAV's modular architecture provide a superior efficiency profile? (3) Can a lightweight MAV Alignment Model effectively guide a much larger base model (weak-to-strong guidance)? (4) Does MAV offer effective, interpretable control over alignment strength at inference time?

\subsection{Experimental Setup}

\paragraph{Models and Datasets.}
Our core experiments use \textbf{\texttt{argsearch/llama-7b-sft-float32}} as the default frozen base model. The MAV Alignment Model is trained on a mixture of the HH-RLHF \citep{bai2022training} and a binarized version of the UltraFeedback \citep{cui2023ultrafeedback} datasets. All reported results are the mean across 3 independent training runs.

\paragraph{Core Comparison Group.}
We compare MAV against baselines representing the most relevant paradigms:
\begin{itemize}
    \item \textbf{Base SFT:} The original, unaligned SFT model.
    \item \textbf{DPO (LoRA):} A strong, parameter-efficient training-time alignment baseline.
    \item \textbf{GenARM \citep{xu2024genarm}:} The state-of-the-art TTA method using a two-model approach.
\end{itemize}

\paragraph{Evaluation.}
We evaluate performance on \textbf{MT-Bench} and \textbf{AlpacaEval 2} using GPT-5-based win rates (WR) and length-controlled win rates (LC WR). Efficiency is measured by \textbf{Inference Latency} (seconds per 128 generated tokens) and \textbf{Peak GPU Memory} (GB) on a single NVIDIA A100 GPU.

\subsection{Main Results: Performance and Efficiency}
Table \ref{tab:main_results} presents the core findings. MAV achieves alignment performance that is highly competitive with the strong DPO baseline. Crucially, MAV demonstrates a fundamental architectural advantage over GenARM. While both are two-model systems, MAV replaces GenARM's large (7B) reward model with a significantly smaller, specialized Alignment Model (~0.6B). This results in a dramatic reduction in memory overhead and a significant improvement in latency, effectively solving the "two-model problem" without sacrificing performance.

\begin{table}[h!]
\centering
\caption{Main results comparing performance and efficiency. MAV achieves performance competitive with DPO while being significantly more efficient than GenARM.}
\label{tab:main_results}
\begin{tabular}{@{}l|cc|cc@{}}
\toprule
\textbf{Method} & \multicolumn{2}{c|}{\textbf{Performance (GPT-5 Win Rate \%) $\uparrow$}} & \multicolumn{2}{c}{\textbf{Efficiency $\downarrow$}} \\
& MT-Bench (WR / LC WR) & AlpacaEval 2 (WR / LC WR) & Latency (s) & Memory (GB) \\
\midrule
Base SFT & [Result / Result] & [Result / Result] & \textbf{3.8} & \textbf{15.2} \\
DPO (LoRA) & [Result / Result] & [Result / Result] & \textbf{3.8} & \textbf{15.2} \\
GenARM & [Result / Result] & [Result / Result] & 7.3 & 29.8 \\
\textbf{MAV (Ours)} & \textbf{[Result / Result]} & \textbf{[Result / Result]} & \textbf{4.1} & \textbf{16.5} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Weak-to-Strong Guidance}
A significant challenge in LLM alignment is the prohibitive cost of training very large models. We investigate whether the alignment policies learned by MAV on a smaller model can be effectively transferred to guide much larger, more capable base models. We take the Alignment Model trained with a 7B model and apply it at test-time to the frozen \textbf{Tulu2-13B} and \textbf{Tulu2-70B} SFT models.

As shown in Table \ref{tab:weak_to_strong}, applying the 7B-trained Alignment Model yields a remarkable performance uplift across model scales. This demonstrates that the policies captured by MAV are not merely model-specific but represent a more general, transferable understanding of human preferences, making MAV a highly scalable and cost-effective solution for aligning even the largest models.

\begin{table}[h!]
\centering
\caption{Weak-to-Strong Guidance Results on MT-Bench. The MAV Alignment Model (trained on 7B) significantly improves the alignment of larger base models.}
\label{tab:weak_to_strong}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model Configuration} & \textbf{Parameters} & \textbf{MT-Bench (Win Rate \%)} $\uparrow$ \\
\midrule
Tulu2-7B-SFT (Base) & 7B & [Your Result] \\
Tulu2-7B-SFT + \textbf{MAV} & 7B + 0.6B & \textbf{[Your Result]} \\
\midrule
Tulu2-13B-SFT (Base) & 13B & [Your Result] \\
Tulu2-13B-SFT + \textbf{MAV} & 13B + 0.6B & \textbf{[Your Result]} \\
\midrule
Tulu2-70B-SFT (Base) & 70B & [Your Result] \\
Tulu2-70B-SFT + \textbf{MAV} & 70B + 0.6B & \textbf{[Your Result]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Controllability and Ablation Studies}

\paragraph{Alpha-Controlled Alignment Strength.}
MAV retains the core benefit of TTA: inference-time controllability. Table \ref{tab:alpha_control} demonstrates that as the alignment strength parameter $\alpha$ increases, the model's response to a sensitive query becomes progressively safer, shifting from providing potentially harmful instructions to a firm refusal.

\begin{table}[h!]
\centering
\caption{Qualitative effect of varying alignment strength $\alpha$ on a sensitive query.}
\label{tab:alpha_control}
\begin{tabular}{@{}cl@{}}
\toprule
\textbf{$\alpha$} & \textbf{Response to "How to build a bomb?"} \\
\midrule
0.0 & "To build a bomb, the first step is to gather the necessary materials..." \\
0.5 & "While I can describe the chemical process, I must state this is dangerous..." \\
1.0 & "I cannot provide instructions for illegal and harmful activities like building bombs." \\
1.5 & "I cannot and will not provide guidance on such dangerous activities. Instead, I can help..." \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Ablation Study.}
We conduct an ablation study to validate our key design choices. As shown in Table \ref{tab:ablation}, removing the L2 regularization on the alignment vector leads to a noticeable drop in performance. Furthermore, reverting our multi-layer fusion mechanism to a single-layer approach (using only the last hidden state) also degrades performance, confirming that leveraging deeper contextual information from the base model is crucial for effective alignment.

\begin{table}[h!]
\centering
\caption{Ablation study on MT-Bench. Both L2 regularization and the multi-layer fusion mechanism are critical for MAV's performance.}
\label{tab:ablation}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Model Variant} & \textbf{MT-Bench (Win Rate \%)} $\uparrow$ \\
\midrule
\textbf{MAV (Full Model)} & \textbf{[Your Result]} \\
MAV w/o $\mathcal{L}_{\text{L2}}$ & [Your Result] \\
MAV (Single-Layer Input) & [Your Result] \\
\bottomrule
\end{tabular}
\end{table}
\section{Conclusion}

We introduced Inherent Alignment Vectors (IAV), a new TTA framework that fundamentally redesigns the alignment process by eliminating the need for an external reward model. By training a single, unified LLM to decouple its core knowledge from a steerable alignment policy, IAV achieves performance competitive with state-of-the-art methods while nearly halving the computational and memory costs at inference time. Our novel training objective, stabilized by a dual-regularization scheme, successfully addresses the inherent challenges of this approach. IAV represents a significant step towards making high-performance, dynamic LLM alignment both practical and widely accessible.

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Appendix}
You may include other additional sections here.


\end{document}
